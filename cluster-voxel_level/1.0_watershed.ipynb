{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watershed for Parcellation\n",
    "1. Use watershed to parcellate activation maps \n",
    "2. generate one map for each region \n",
    "3. find the local maxima of each region and mni coordinate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from algo import *\n",
    "from skimage import segmentation\n",
    "from scipy import ndimage\n",
    "from collections import Counter\n",
    "from nilearn import image, plotting, datasets, surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relavant Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_local_maximum(image, labels=None, footprint=None):\n",
    "    \"\"\"Return a boolean array of points that are local maxima\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    image: ndarray (2-D, 3-D, ...)\n",
    "        intensity image\n",
    "        \n",
    "    labels: ndarray, optional \n",
    "        find maxima only within labels. Zero is reserved for background.\n",
    "    \n",
    "    footprint: ndarray of bools, optional\n",
    "        binary mask indicating the neighborhood to be examined\n",
    "        `footprint` must be a matrix with odd dimensions, the center is taken \n",
    "        to be the point in question.\n",
    "    Returns\n",
    "    -------\n",
    "    result: ndarray of bools\n",
    "        mask that is True for pixels that are local maxima of `image`\n",
    "    Notes\n",
    "    -----\n",
    "    This function is copied from watershed module in CellProfiler.\n",
    "    This module implements a watershed algorithm that apportions pixels into\n",
    "    marked basins. The algorithm uses a priority queue to hold the pixels\n",
    "    with the metric for the priority queue being pixel value, then the time\n",
    "    of entry into the queue - this settles ties in favor of the closest marker.\n",
    "    Some ideas taken from\n",
    "    Soille, \"Automated Basin Delineation from Digital Elevation Models Using\n",
    "    Mathematical Morphology\", Signal Processing 20 (1990) 171-182.\n",
    "    The most important insight in the paper is that entry time onto the queue\n",
    "    solves two problems: a pixel should be assigned to the neighbor with the\n",
    "    largest gradient or, if there is no gradient, pixels on a plateau should\n",
    "    be split between markers on opposite sides.\n",
    "    Originally part of CellProfiler, code licensed under both GPL and BSD licenses.\n",
    "    Website: http://www.cellprofiler.org\n",
    "    Copyright (c) 2003-2009 Massachusetts Institute of Technology\n",
    "    Copyright (c) 2009-2011 Broad Institute\n",
    "    All rights reserved.\n",
    "    Original author: Lee Kamentsky\n",
    "    Examples\n",
    "    --------\n",
    "    >>> image = np.zeros((4, 4))\n",
    "    >>> image[1, 2] = 2\n",
    "    >>> image[3, 3] = 1\n",
    "    >>> image\n",
    "    array([[ 0.,  0.,  0.,  0.],\n",
    "           [ 0.,  0.,  2.,  0.],\n",
    "           [ 0.,  0.,  0.,  0.],\n",
    "           [ 0.,  0.,  0.,  1.]])\n",
    "    >>> is_local_maximum(image)\n",
    "    array([[ True, False, False, False],\n",
    "           [ True, False,  True, False],\n",
    "           [ True, False, False, False],\n",
    "           [ True,  True, False,  True]], dtype='bool')\n",
    "    >>> image = np.arange(16).reshape((4, 4))\n",
    "    >>> labels = np.array([[1, 2], [3, 4]])\n",
    "    >>> labels = np.repeat(np.repeat(labels, 2, axis=0), 2, axis=1)\n",
    "    >>> labels\n",
    "    array([[1, 1, 2, 2],\n",
    "           [1, 1, 2, 2],\n",
    "           [3, 3, 4, 4],\n",
    "           [3, 3, 4, 4]])\n",
    "    >>> image\n",
    "    array([[ 0,  1,  2,  3],\n",
    "           [ 4,  5,  6,  7],\n",
    "           [ 8,  9, 10, 11],\n",
    "           [12, 13, 14, 15]])\n",
    "    >>> is_local_maximum(image, labels=labels)\n",
    "    array([[False, False, False, False],\n",
    "           [False,  True, False,  True],\n",
    "           [False, False, False, False],\n",
    "           [False,  True, False,  True]], dtype='bool')\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        labels = np.ones(image.shape, dtype=np.uint8)\n",
    "    if footprint is None:\n",
    "        footprint = np.ones([3] * image.ndim, dtype=np.uint8)\n",
    "    assert((np.all(footprint.shape) & 1) == 1)\n",
    "    footprint = (footprint != 0)\n",
    "    footprint_extent = (np.array(footprint.shape)-1) // 2\n",
    "    if np.all(footprint_extent == 0):\n",
    "        return labels > 0\n",
    "    result = (labels > 0).copy()\n",
    "    #\n",
    "    # Create a labels matrix with zeros at the borders that might be\n",
    "    # hit by the footprint.\n",
    "    #\n",
    "    big_labels = np.zeros(np.array(labels.shape) + footprint_extent*2,\n",
    "                          labels.dtype)\n",
    "    big_labels[[slice(fe,-fe) for fe in footprint_extent]] = labels\n",
    "    #\n",
    "    # Find the relative indexes of each footprint element\n",
    "    #\n",
    "    image_strides = np.array(image.strides) // image.dtype.itemsize\n",
    "    big_strides = np.array(big_labels.strides) // big_labels.dtype.itemsize\n",
    "    result_strides = np.array(result.strides) // result.dtype.itemsize\n",
    "    footprint_offsets = np.mgrid[[slice(-fe,fe+1) for fe in footprint_extent]]\n",
    "    \n",
    "    fp_image_offsets = np.sum(image_strides[:, np.newaxis] *\n",
    "                              footprint_offsets[:, footprint], 0)\n",
    "    fp_big_offsets = np.sum(big_strides[:, np.newaxis] *\n",
    "                            footprint_offsets[:, footprint], 0)\n",
    "    #\n",
    "    # Get the index of each labeled pixel in the image and big_labels arrays\n",
    "    #\n",
    "    indexes = np.mgrid[[slice(0,x) for x in labels.shape]][:, labels > 0]\n",
    "    image_indexes = np.sum(image_strides[:, np.newaxis] * indexes, 0)\n",
    "    big_indexes = np.sum(big_strides[:, np.newaxis] * \n",
    "                         (indexes + footprint_extent[:, np.newaxis]), 0)\n",
    "    result_indexes = np.sum(result_strides[:, np.newaxis] * indexes, 0)\n",
    "    #\n",
    "    # Now operate on the raveled images\n",
    "    #\n",
    "    big_labels_raveled = big_labels.ravel()\n",
    "    image_raveled = image.ravel()\n",
    "    result_raveled = result.ravel()\n",
    "    #\n",
    "    # A hit is a hit if the label at the offset matches the label at the pixel\n",
    "    # and if the intensity at the pixel is greater or equal to the intensity\n",
    "    # at the offset.\n",
    "    #\n",
    "    for fp_image_offset, fp_big_offset in zip(fp_image_offsets, fp_big_offsets):\n",
    "        same_label = (big_labels_raveled[big_indexes + fp_big_offset] ==\n",
    "                      big_labels_raveled[big_indexes])\n",
    "        less_than = (image_raveled[image_indexes[same_label]] <\n",
    "                     image_raveled[image_indexes[same_label]+ fp_image_offset])\n",
    "        result_raveled[result_indexes[same_label][less_than]] = False\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watershed(data, sigma=1, thresh=0, seeds=None):\n",
    "    \"\"\"The implementation of watershed algorithm.\"\"\"\n",
    "    \n",
    "    thresh = thresh > 0 and thresh\n",
    "    if thresh == 0:\n",
    "        mask = data > thresh \n",
    "    else:\n",
    "        mask = data >= thresh\n",
    "    data = ndimage.gaussian_filter(data, sigma)\n",
    "    if seeds is None:\n",
    "        # using unmasked data to get local maximum\n",
    "        seeds = is_local_maximum(data)\n",
    "    # mask out those smaller than threshold\n",
    "    seeds[~mask] = 0\n",
    "\n",
    "    se = ndimage.generate_binary_structure(3, 3)\n",
    "    markers = ndimage.label(seeds, se)[0]\n",
    "    \n",
    "    seg_input =  mask\n",
    "\n",
    "    result = segmentation.watershed(seg_input, markers, mask=mask)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mni(data_array):\n",
    "    \"\"\"\n",
    "    convert array(beta) to mni space\n",
    "    specific for the analysis\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    data_array: ndarray\n",
    "        1D array, contraining beta value\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    mni_data: Nifti1Image\n",
    "        NiftiImage, with space and afine specific for this analysis\n",
    "    \"\"\"\n",
    "    # acquire sample parameters\n",
    "    sample_path = '/Users/anmin/Documents/attsig_data/data/beta_0001.nii'\n",
    "    sample_nii = nib.load(sample_path)\n",
    "    ori_shape = np.array(sample_nii.get_data()).shape\n",
    "    affine = sample_nii.affine.copy()\n",
    "    hdr = sample_nii.header.copy()\n",
    "\n",
    "    del sample_nii\n",
    "\n",
    "    mni_data = nib.Nifti1Image(data_array.reshape(ori_shape),affine,hdr)\n",
    "\n",
    "    return mni_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/anmin/Documents/attsig_data/'\n",
    "\n",
    "fa = image.get_data(os.path.join(root_path,\n",
    "                            'nii_file',\n",
    "                            'fa_fdr_corrected.nii.gz'))\n",
    "    \n",
    "sa = image.get_data(os.path.join(root_path,\n",
    "                            'nii_file',\n",
    "                            'sa_fdr_corrected.nii.gz'))\n",
    "\n",
    "mask_nan = image.get_data(os.path.join(root_path, 'nii_file',\n",
    "                                        'mask_nan.nii.gz'))\n",
    "fa_mask = np.load(os.path.join(root_path, \n",
    "                                'data', 'statistical_masks',\n",
    "                                'fa_fdr.npy'))\n",
    "sa_mask = np.load(os.path.join(root_path, \n",
    "                                'data', 'statistical_masks',\n",
    "                                'sa_fdr.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(root_path,\n",
    "                        'nii_file', 'cluster_map_small_version')\n",
    "# the original cluster parcelation is in the file cluster map\n",
    "# in the file small version, the cluster for SA is smaller "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read HO map mapper \n",
    "json_path = os.path.join(root_path,\n",
    "                        'labelmapper.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(json_path)\n",
    "map_lst = json.load(f)\n",
    "cat_num = 49\n",
    "\n",
    "mapper = {}\n",
    "for i in range(cat_num):\n",
    "    mapper[i] = map_lst[i][2] # here the mapper is changed to abreviation of the region \n",
    "                            # instead of the full name of the region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watershed Parcellation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.isnan(fa)\n",
    "\n",
    "# here the assumaptuon is the sign of weight is irrelevant, but rather\n",
    "# the intensity\n",
    "fa = abs(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fa\n",
    "thresh = 0\n",
    "mask = data > thresh \n",
    "sigma = 0.1\n",
    "data = ndimage.gaussian_filter(data, sigma)\n",
    "seeds = is_local_maximum(data)\n",
    "seeds[~mask] = 0\n",
    "se = ndimage.generate_binary_structure(3, 3)\n",
    "markers = ndimage.label(seeds, se)[0]\n",
    "seg_input = mask\n",
    "\n",
    "parcel_map = segmentation.watershed(seg_input, markers, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_loi(p_map, mask, v_num):\n",
    "    \"\"\"\n",
    "    find the cluster label given minimum cluster size\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p_map : ndarray 3D \n",
    "        activation map\n",
    "    mask : ndarray 3D\n",
    "        relavant voxels, same shape with p_map\n",
    "    v_num : int\n",
    "        minimun size of a cluster \n",
    "    \n",
    "    Retruns\n",
    "    -------\n",
    "    selected_cluster : list of tuple\n",
    "        (label, size)\n",
    "    \"\"\"\n",
    "    counter = Counter(p_map[mask])\n",
    "\n",
    "    selected_loi = []\n",
    "    for key, val in counter.items():\n",
    "        if val > v_num:\n",
    "            selected_loi.append((key, val))\n",
    "    \n",
    "    return selected_loi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_cat(mask, parcel_map):\n",
    "    p_map = parcel_map.flatten()\n",
    "    for i in range(mask.size):\n",
    "        if not mask[i]:\n",
    "            p_map[i] = np.nan \n",
    "    return p_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_loi = find_loi(parcel_map, mask, 10) # the threshold is set to 10 instead of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_loi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_map = parcel_map.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate nii file for each label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, num in selected_loi: \n",
    "    if label == 0:\n",
    "        continue\n",
    "    mask_l = parcel_map.flatten() == label\n",
    "\n",
    "    mni_vec = mask_cat(mask_l, parcel_map.flatten())\n",
    "    nif_fa = to_mni(mni_vec)\n",
    "\n",
    "    nib.save(nif_fa, \n",
    "         os.path.join(save_path, 'fa',\n",
    "                    f'{label}_{num}_fa.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate nii file for every label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen selectec mask \n",
    "mni_vec = []\n",
    "p_map = parcel_map.flatten()\n",
    "selected_cat = [i for i, j in selected_loi]\n",
    "\n",
    "for val in p_map:\n",
    "    if val in selected_cat:\n",
    "        mni_vec.append(val)\n",
    "    else:\n",
    "        mni_vec.append(np.nan)\n",
    "\n",
    "        \n",
    "nif_fa = to_mni(np.array(mni_vec))\n",
    "nib.save(nif_fa, \n",
    "         os.path.join(save_path, 'fa',\n",
    "                    f'full_fa.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ho = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "ho_map = image.get_data(dataset_ho['maps']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niimg = datasets.load_mni152_template()\n",
    "cortex_mask = ~(ho_map == 0).flatten() # ristrict maximum value in cortex \n",
    "table_dic = {}\n",
    "total_num = len(selected_loi)\n",
    "\n",
    "for label, num in selected_loi:\n",
    "    start = time.time()\n",
    "    if label == 0:\n",
    "        continue    \n",
    "    table_dic[label] = {}\n",
    "    mask = image.get_data(os.path.join(save_path, 'fa',\n",
    "                        f'{label}_{num}_fa.nii.gz'))\n",
    "    mask = mask == label\n",
    "    model_shape = mask.shape\n",
    "    mask = mask.flatten()\n",
    "    mask_with_cortex = mask * cortex_mask\n",
    "    fa_arr = fa.flatten()\n",
    "\n",
    "    # get beta values specific to label\n",
    "    arr_to_max = []\n",
    "    for i, val in enumerate(mask_with_cortex):\n",
    "        if val:\n",
    "            arr_to_max.append(fa_arr[i])\n",
    "        else:\n",
    "            arr_to_max.append(np.nan)\n",
    "\n",
    "    arr_to_max = np.array(arr_to_max).reshape(model_shape)\n",
    "    arr_to_max = np.nan_to_num(arr_to_max)\n",
    "\n",
    "    # get cluster size\n",
    "    table_dic[label]['voxel_size'] = num\n",
    "\n",
    "    # get matrix coordinates\n",
    "    x, y, z = np.unravel_index(np.argmax(arr_to_max), arr_to_max.shape)\n",
    "    table_dic[label]['voxel_location'] = (x, y, z)\n",
    "\n",
    "    # get mni coordinates\n",
    "    table_dic[label]['mni_coordinates'] = image.coord_transform(x, y, z, \n",
    "                                                            niimg.affine)\n",
    "    # get the right hemisphere or left hemisphere   \n",
    "    hem = 'left ' if table_dic[label]['mni_coordinates'][0] < 0 else 'right '\n",
    " \n",
    "    # get the label of H-O atlas \n",
    "    table_dic[label]['HO_categor_by_peak'] = ho_map[x, y, z]\n",
    "\n",
    "    # get name of the cluster \n",
    "    names = mapper[table_dic[label]['HO_categor_by_peak']]\n",
    "    table_dic[label]['names_by_peak'] = hem + names\n",
    "\n",
    "    # sanity check, if the peak is in the left hemisphere\n",
    "    if hem == 'left ':\n",
    "        table_dic[label]['HO_categor_by_peak'] = -1 * ho_map[x, y, z]\n",
    "\n",
    "    # get cluster name by covered region \n",
    "    label_count_arr = ho_map[mask.reshape(model_shape)]\n",
    "    count_dic = Counter(label_count_arr)\n",
    "    total = sum(count_dic.values())\n",
    "\n",
    "    cat_percentile = {}\n",
    "    for k in count_dic.keys():\n",
    "        cat_percentile[k] = count_dic[k] / total \n",
    "    cat_percentile = sorted(cat_percentile.items(), key=lambda x: -x[1])\n",
    "\n",
    "    table_dic[label]['percentile_distribution'] = cat_percentile\n",
    "    table_dic[label]['HO_categor_by_area'] = cat_percentile[0][0]\n",
    "    table_dic[label]['names_by_area'] = hem + mapper[cat_percentile[0][0]]\n",
    "\n",
    "    # get intensity \n",
    "    table_dic[label]['z-score'] = (np.mean(fa_arr[mask]), np.std(fa_arr[mask]))\n",
    "\n",
    "    # printout\n",
    "    end = time.time()\n",
    "    print('--------------------------------------------')\n",
    "    print(f'{label} finished, time used: {end - start:.2f} seconds, ')\n",
    "    total_num -= 1\n",
    "    print(f'{total_num} left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fa = pd.DataFrame(table_dic).transpose()\n",
    "\n",
    "df_fa.to_csv(os.path.join(root_path,\n",
    "                        'cluster_table_small_version',\n",
    "                        'fa_attributes.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modification of Clusters\n",
    "1. Directly read file\n",
    "2. the standard of cluster label is 'HO_categor_by_peak'\n",
    "3. merge clusters with the same label to the finall nii file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fa = pd.read_csv(os.path.join(root_path, 'cluster_table_small_version',\n",
    "                                'fa_attributes.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = np.unique(df_fa['HO_categor_by_peak'])\n",
    "fa_arr = fa.flatten()\n",
    "\n",
    "total_num = len(cats)\n",
    "\n",
    "for cat in cats:\n",
    "    if cat == 0: # drop background\n",
    "        continue\n",
    "\n",
    "    start = time.time()\n",
    "    base_arr = np.zeros((fa_arr.size))\n",
    "    df_temp = df_fa[df_fa['HO_categor_by_peak'] == cat ]\n",
    "    for _, row in df_temp.iterrows():\n",
    "        label = row['Unnamed: 0']\n",
    "        num = row['voxel_size']\n",
    "        mask = image.get_data(os.path.join(save_path, 'fa',\n",
    "                        f'{label}_{num}_fa.nii.gz')).flatten()\n",
    "        \n",
    "        for i, val in enumerate(mask):\n",
    "            if not np.isnan(val):\n",
    "                base_arr[i] = cat\n",
    "    \n",
    "    for i, val in enumerate(base_arr):\n",
    "        if val == 0:\n",
    "            base_arr[i] = np.nan\n",
    "\n",
    "    \n",
    "    voxel_num = np.sum(~np.isnan(base_arr))\n",
    "    base_arr = base_arr.reshape(fa.shape)\n",
    "    nif_fa = to_mni(base_arr)\n",
    "\n",
    "    nib.save(nif_fa, \n",
    "         os.path.join(save_path, 'fa_screened',\n",
    "                    f'{cat}_{voxel_num}_fa.nii.gz'))\n",
    "    end = time.time()\n",
    "    print('--------------------------------------------')\n",
    "    print(f'{cat} finished, time used: {end - start:.2f} seconds, ')\n",
    "    total_num -= 1\n",
    "    print(f'{total_num} left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct New Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(save_path, \n",
    "                    'fa_screened')\n",
    "files = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df = pd.read_csv(os.path.join(root_path, 'cluster_table_small_version',\n",
    "                                'fa_attributes.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic_new = {}\n",
    "for file in files:\n",
    "    cat, num = int(file.split('_')[0]), int(file.split('_')[1])\n",
    "    df_dic_new[cat] = {}\n",
    "\n",
    "    mask = image.get_data(os.path.join(save_path, 'fa_screened',\n",
    "                        file))\n",
    "    mask = mask == cat\n",
    "    model_shape = mask.shape\n",
    "    mask = mask.flatten()\n",
    "    mask_with_cortex = mask * cortex_mask\n",
    "    fa_arr = fa.flatten()\n",
    "\n",
    "    # get beta values specific to label\n",
    "    arr_to_max = []\n",
    "    for i, val in enumerate(mask_with_cortex):\n",
    "        if val:\n",
    "            arr_to_max.append(fa_arr[i])\n",
    "        else:\n",
    "            arr_to_max.append(np.nan)\n",
    "\n",
    "    arr_to_max = np.array(arr_to_max).reshape(model_shape)\n",
    "    arr_to_max = np.nan_to_num(arr_to_max)\n",
    "\n",
    "    # get number of voxels\n",
    "    df_dic_new[cat]['number_of_voxels'] = num \n",
    "\n",
    "    # get name of cluster \n",
    "    name_df = ref_df[ref_df['HO_categor_by_peak'] == cat]\n",
    "    names = name_df['names_by_peak'].iloc[0]\n",
    "    df_dic_new[cat]['cluster_name'] = names \n",
    "\n",
    "    # get mni coordinates\n",
    "    x, y, z = np.unravel_index(np.argmax(arr_to_max), arr_to_max.shape)\n",
    "    df_dic_new[cat]['mni_coordinates'] = image.coord_transform(x, y, z, \n",
    "                                                                niimg.affine)\n",
    "\n",
    "    # get intensity \n",
    "    df_dic_new[cat]['z-score'] = (np.mean(fa_arr[mask]), np.std(fa_arr[mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fa_new = pd.DataFrame(df_dic_new).transpose()\n",
    "\n",
    "df_fa_new.to_csv(os.path.join(root_path,\n",
    "                        'cluster_table_small_version',\n",
    "                        'fa_integrated.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watershed Parcellation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.isnan(sa)\n",
    "sa = abs(sa)\n",
    "\n",
    "data = sa\n",
    "thresh = 0\n",
    "mask = data > thresh \n",
    "sigma = 0.1\n",
    "data = ndimage.gaussian_filter(data, sigma)\n",
    "seeds = is_local_maximum(data)\n",
    "seeds[~mask] = 0\n",
    "se = ndimage.generate_binary_structure(3, 3)\n",
    "markers = ndimage.label(seeds, se)[0]\n",
    "seg_input = mask\n",
    "\n",
    "parcel_map = segmentation.watershed(seg_input, markers, mask=mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_loi = find_loi(parcel_map, mask, 10) # threshold changed from 50 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_map = parcel_map.astype('float') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate nii file for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, num in selected_loi: \n",
    "    if label == 0: \n",
    "        continue\n",
    "    mask_l = parcel_map.flatten() == label\n",
    "\n",
    "    mni_vec = mask_cat(mask_l, parcel_map.flatten())\n",
    "    nif_sa = to_mni(mni_vec)\n",
    "\n",
    "    nib.save(nif_sa, \n",
    "         os.path.join(save_path, 'sa',\n",
    "                    f'{label}_{num}_sa.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen selectec mask \n",
    "mni_vec = []\n",
    "p_map = parcel_map.flatten()\n",
    "selected_cat = [i for i, j in selected_loi]\n",
    "\n",
    "for val in p_map:\n",
    "    if val in selected_cat:\n",
    "        mni_vec.append(val)\n",
    "    else:\n",
    "        mni_vec.append(np.nan)\n",
    "\n",
    "        \n",
    "nif_fa = to_mni(np.array(mni_vec))\n",
    "nib.save(nif_fa, \n",
    "         os.path.join(save_path, 'sa',\n",
    "                    f'full_sa.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dic = {}\n",
    "\n",
    "for label, num in selected_loi:\n",
    "    if label == 0:\n",
    "        continue\n",
    "    table_dic[label] = {}\n",
    "    mask = image.get_data(os.path.join(save_path, 'sa',\n",
    "                        f'{label}_{num}_sa.nii.gz'))\n",
    "    mask = mask == label\n",
    "    model_shape = mask.shape\n",
    "    mask = mask.flatten()\n",
    "    mask_with_cortex = mask * cortex_mask\n",
    "    sa_arr = sa.flatten()\n",
    "\n",
    "    # get beta values specific to label\n",
    "    arr_to_max = []\n",
    "    for i, val in enumerate(mask_with_cortex):\n",
    "        if val:\n",
    "            arr_to_max.append(fa_arr[i])\n",
    "        else:\n",
    "            arr_to_max.append(np.nan)\n",
    "\n",
    "    arr_to_max = np.array(arr_to_max).reshape(model_shape)\n",
    "    arr_to_max = np.nan_to_num(arr_to_max)\n",
    "\n",
    "    # get cluster size\n",
    "    table_dic[label]['voxel_size'] = num\n",
    "\n",
    "    # get matrix coordinates\n",
    "    x, y, z = np.unravel_index(np.argmax(arr_to_max), arr_to_max.shape)\n",
    "    table_dic[label]['voxel_location'] = (x, y, z)\n",
    "\n",
    "    # get mni coordinates\n",
    "    table_dic[label]['mni_coordinates'] = image.coord_transform(x, y, z, \n",
    "                                                            niimg.affine)\n",
    "    # get the right hemisphere or left hemisphere   \n",
    "    hem = 'left ' if table_dic[label]['mni_coordinates'][0] < 0 else 'right '\n",
    "\n",
    "    # get the label of H-O atlas \n",
    "    table_dic[label]['HO_categor_by_peak'] = ho_map[x, y, z]\n",
    "\n",
    "    # get name of the cluster \n",
    "    names = mapper[table_dic[label]['HO_categor_by_peak']]\n",
    "    table_dic[label]['names_by_peak'] = hem + names\n",
    "\n",
    "    # sanity check, if the peak is in the left hemisphere\n",
    "    if hem == 'left ':\n",
    "        table_dic[label]['HO_categor_by_peak'] = -1 * ho_map[x, y, z]\n",
    "\n",
    "    # get cluster name by covered region \n",
    "    label_count_arr = ho_map[mask.reshape(model_shape)]\n",
    "    count_dic = Counter(label_count_arr)\n",
    "    total = sum(count_dic.values())\n",
    "\n",
    "    cat_percentile = {}\n",
    "    for k in count_dic.keys():\n",
    "        cat_percentile[k] = count_dic[k] / total \n",
    "    cat_percentile = sorted(cat_percentile.items(), key=lambda x: -x[1])\n",
    "\n",
    "    table_dic[label]['percentile_distribution'] = cat_percentile\n",
    "    table_dic[label]['HO_categor_by_area'] = cat_percentile[0][0]\n",
    "    table_dic[label]['names_by_area'] = mapper[cat_percentile[0][0]]\n",
    "\n",
    "    # get intensity \n",
    "    table_dic[label]['z-score'] = (np.mean(sa_arr[mask]), np.std(sa_arr[mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sa = pd.DataFrame(table_dic).transpose()\n",
    "\n",
    "df_sa.to_csv(os.path.join(root_path,\n",
    "                        'cluster_table_small_version',\n",
    "                        'sa_attributes.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modification of Clusters\n",
    "1. Read the modified cluster csv file that is screened manually\n",
    "2. the standard of cluster label is 'HO_categor_by_peak'\n",
    "3. merge clusters with the same label to the finall nii file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sa = pd.read_csv(os.path.join(root_path, 'cluster_table_small_version',\n",
    "                                'sa_attributes.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = np.unique(df_sa['HO_categor_by_peak'])\n",
    "sa_arr = sa.flatten()\n",
    "\n",
    "for cat in cats:\n",
    "    if cat == 0:\n",
    "        continue    \n",
    "    base_arr = np.zeros((sa_arr.size))\n",
    "    df_temp = df_sa[df_sa['HO_categor_by_peak'] == cat ]\n",
    "    for _, row in df_temp.iterrows():\n",
    "        label = row['Unnamed: 0']\n",
    "        num = row['voxel_size']\n",
    "        mask = image.get_data(os.path.join(save_path, 'sa',\n",
    "                        f'{label}_{num}_sa.nii.gz')).flatten()\n",
    "        \n",
    "        for i, val in enumerate(mask):\n",
    "            if not np.isnan(val):\n",
    "                base_arr[i] = cat\n",
    "    \n",
    "    for i, val in enumerate(base_arr):\n",
    "        if val == 0:\n",
    "            base_arr[i] = np.nan\n",
    "\n",
    "    \n",
    "    voxel_num = np.sum(~np.isnan(base_arr))\n",
    "    base_arr = base_arr.reshape(sa.shape)\n",
    "    nif_sa = to_mni(base_arr)\n",
    "\n",
    "    nib.save(nif_sa, \n",
    "         os.path.join(save_path, 'sa_screened',\n",
    "                    f'{cat}_{voxel_num}_sa.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct New Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(save_path, \n",
    "                    'sa_screened')\n",
    "files = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df = pd.read_csv(os.path.join(root_path, 'cluster_table_small_version',\n",
    "                                'sa_attributes.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic_new = {}\n",
    "for file in files:\n",
    "    if file == '.DS_Store':\n",
    "        continue\n",
    "    cat, num = int(file.split('_')[0]), int(file.split('_')[1])\n",
    "    df_dic_new[cat] = {}\n",
    "\n",
    "    mask = image.get_data(os.path.join(save_path, 'sa_screened',\n",
    "                        file))\n",
    "    mask = mask == cat\n",
    "    model_shape = mask.shape\n",
    "    mask = mask.flatten()\n",
    "    mask_with_cortex = mask * cortex_mask\n",
    "    sa_arr = sa.flatten()\n",
    "\n",
    "    # get beta values specific to label\n",
    "    arr_to_max = []\n",
    "    for i, val in enumerate(mask_with_cortex):\n",
    "        if val:\n",
    "            arr_to_max.append(sa_arr[i])\n",
    "        else:\n",
    "            arr_to_max.append(np.nan)\n",
    "\n",
    "    arr_to_max = np.array(arr_to_max).reshape(model_shape)\n",
    "    arr_to_max = np.nan_to_num(arr_to_max)\n",
    "\n",
    "    # get number of voxels\n",
    "    df_dic_new[cat]['number_of_voxels'] = num \n",
    "\n",
    "    # get name of cluster \n",
    "    name_df = ref_df[ref_df['HO_categor_by_peak'] == cat]\n",
    "    names = name_df['names_by_peak'].iloc[0]\n",
    "    df_dic_new[cat]['cluster_name'] = names \n",
    "\n",
    "    # get mni coordinates\n",
    "    x, y, z = np.unravel_index(np.argmax(arr_to_max), arr_to_max.shape)\n",
    "    df_dic_new[cat]['mni_coordinates'] = image.coord_transform(x, y, z, \n",
    "                                                                niimg.affine)\n",
    "\n",
    "    # get intensity \n",
    "    df_dic_new[cat]['z-score'] = (np.mean(sa_arr[mask]), np.std(sa_arr[mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sa = pd.DataFrame(df_dic_new).transpose()\n",
    "\n",
    "df_sa.to_csv(os.path.join(root_path,\n",
    "                        'cluster_table_small_version',\n",
    "                        'sa_integrated.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b11f27ed2f832c078bc513e9a1cf515193e964197e9fe67820b39705f67be7ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
